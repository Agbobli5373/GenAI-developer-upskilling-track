{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Prompt Engineering Playground\n",
        "\n",
        "# This notebook demonstrates various prompting techniques including:\n",
        "# - Zero-shot prompting\n",
        "# - Few-shot prompting\n",
        "# - Chain-of-thought prompting\n",
        "\n",
        "# These techniques are applied to diverse tasks:\n",
        "# - Arithmetic problems\n",
        "# - Text rephrasing\n",
        "# - Content summarization\n",
        "# - Classification tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting openai==0.28.0 (from -r requirements.txt (line 1))\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pandas==2.0.0 (from -r requirements.txt (line 2))\n",
            "  Downloading pandas-2.0.0.tar.gz (5.3 MB)\n",
            "     ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.3/5.3 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 0.5/5.3 MB 1.9 MB/s eta 0:00:03\n",
            "     ------- -------------------------------- 1.0/5.3 MB 2.0 MB/s eta 0:00:03\n",
            "     ----------- ---------------------------- 1.6/5.3 MB 2.0 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 2.1/5.3 MB 2.2 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 2.9/5.3 MB 2.4 MB/s eta 0:00:02\n",
            "     ----------------------- ---------------- 3.1/5.3 MB 2.4 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 3.9/5.3 MB 2.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  5.2/5.3 MB 2.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 5.3/5.3 MB 2.6 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: still running...\n",
            "  Getting requirements to build wheel: still running...\n",
            "  Getting requirements to build wheel: still running...\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting matplotlib==3.7.1 (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.7.1.tar.gz (38.0 MB)\n",
            "     ---------------------------------------- 0.0/38.0 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.5/38.0 MB 3.4 MB/s eta 0:00:12\n",
            "     - -------------------------------------- 1.6/38.0 MB 3.8 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 2.4/38.0 MB 4.1 MB/s eta 0:00:09\n",
            "     --- ------------------------------------ 3.4/38.0 MB 4.3 MB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 4.7/38.0 MB 4.5 MB/s eta 0:00:08\n",
            "     ------ --------------------------------- 5.8/38.0 MB 4.6 MB/s eta 0:00:07\n",
            "     ------- -------------------------------- 6.8/38.0 MB 4.8 MB/s eta 0:00:07\n",
            "     -------- ------------------------------- 8.1/38.0 MB 4.9 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 9.2/38.0 MB 5.0 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 10.5/38.0 MB 5.1 MB/s eta 0:00:06\n",
            "     ------------ --------------------------- 11.8/38.0 MB 5.2 MB/s eta 0:00:06\n",
            "     -------------- ------------------------- 13.4/38.0 MB 5.4 MB/s eta 0:00:05\n",
            "     --------------- ------------------------ 14.4/38.0 MB 5.4 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 15.2/38.0 MB 5.3 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 16.0/38.0 MB 5.3 MB/s eta 0:00:05\n",
            "     ----------------- ---------------------- 17.0/38.0 MB 5.2 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 17.8/38.0 MB 5.0 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 18.4/38.0 MB 4.9 MB/s eta 0:00:05\n",
            "     -------------------- ------------------- 19.4/38.0 MB 4.9 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 20.7/38.0 MB 5.0 MB/s eta 0:00:04\n",
            "     ---------------------- ----------------- 21.5/38.0 MB 5.0 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 22.5/38.0 MB 4.9 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 23.3/38.0 MB 4.9 MB/s eta 0:00:04\n",
            "     ------------------------- -------------- 24.4/38.0 MB 4.9 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 25.4/38.0 MB 4.9 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 26.5/38.0 MB 4.9 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 27.5/38.0 MB 4.9 MB/s eta 0:00:03\n",
            "     ------------------------------ --------- 28.6/38.0 MB 4.9 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 29.9/38.0 MB 4.9 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 30.9/38.0 MB 4.9 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 32.0/38.0 MB 5.0 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 32.5/38.0 MB 4.9 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 33.6/38.0 MB 4.9 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 34.6/38.0 MB 4.9 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 35.4/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 36.7/38.0 MB 4.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.5/38.0 MB 4.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.7/38.0 MB 4.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 38.0/38.0 MB 3.8 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 4))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting jupyter==1.0.0 (from -r requirements.txt (line 5))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting ipykernel==6.22.0 (from -r requirements.txt (line 6))\n",
            "  Downloading ipykernel-6.22.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting numpy==1.24.3 (from -r requirements.txt (line 7))\n",
            "  Downloading numpy-1.24.3.tar.gz (10.9 MB)\n",
            "     ---------------------------------------- 0.0/10.9 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 1.0/10.9 MB 5.6 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 1.8/10.9 MB 5.0 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 3.4/10.9 MB 5.6 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 4.7/10.9 MB 5.7 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 5.8/10.9 MB 5.7 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 6.8/10.9 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 7.9/10.9 MB 5.6 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 8.7/10.9 MB 5.4 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 10.0/10.9 MB 5.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  10.7/10.9 MB 5.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  10.7/10.9 MB 5.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 10.9/10.9 MB 4.8 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: C:\\Users\\HomePC\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 106, in _run_wrapper\n",
            "    status = _inner_run()\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 97, in _inner_run\n",
            "    return self.run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 386, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 174, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 162, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "                       ^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 53, in _iter_built\n",
            "    candidate = func()\n",
            "                ^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 187, in _make_candidate_from_link\n",
            "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 233, in _make_base_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "                                       ^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in __init__\n",
            "    super().__init__(\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 159, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 236, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 315, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 527, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 642, in _prepare_linked_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 72, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 56, in prepare_distribution_metadata\n",
            "    self._install_build_reqs(finder)\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 126, in _install_build_reqs\n",
            "    build_reqs = self._get_build_requires_wheel()\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 103, in _get_build_requires_wheel\n",
            "    return backend.get_requires_for_build_wheel()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 702, in get_requires_for_build_wheel\n",
            "    return super().get_requires_for_build_wheel(config_settings=cs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 196, in get_requires_for_build_wheel\n",
            "    return self._call_hook(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 402, in _call_hook\n",
            "    raise BackendUnavailable(\n",
            "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Cannot import 'setuptools.build_meta'\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtime\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Union, Optional\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Union, Optional\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Check if we have an API key set in the environment\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # Load environment variables from .env file if present\n",
        "except ImportError:\n",
        "    print(\"dotenv not installed. Using environment variables directly.\")\n",
        "\n",
        "# Set up OpenAI API (you can switch to another LLM provider if needed)\n",
        "import openai\n",
        "\n",
        "# Initialize the Gemini client using the OpenAI library\n",
        "gemini_api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "client = None # Global client variable\n",
        "\n",
        "if not gemini_api_key:\n",
        "    print(\"Warning: GEMINI_API_KEY not found in environment variables. API calls will likely fail.\")\n",
        "else:\n",
        "    try:\n",
        "        client = openai.OpenAI(\n",
        "            api_key=gemini_api_key,\n",
        "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" # From your Gemini sample\n",
        "        )\n",
        "        print(\"Gemini client initialized successfully using OpenAI library.\")\n",
        "    except AttributeError:\n",
        "        print(\"Error: openai.OpenAI class not found. Please ensure you have the 'openai' library version 1.0.0 or higher installed (e.g., pip install --upgrade openai).\")\n",
        "        print(\"API calls will likely fail.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Gemini client: {e}\")\n",
        "        print(\"API calls will likely fail.\")\n",
        "\n",
        "# Function to handle API calls with retry logic\n",
        "def get_completion(\n",
        "    prompt: str, \n",
        "    model: str = \"gemini-2.0-flash\",  # Changed default model to match your Gemini sample\n",
        "    temperature: float = 0,\n",
        "    max_retries: int = 3,\n",
        "    retry_delay: int = 2\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the Gemini API (via OpenAI compatible endpoint) and get a completion.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The prompt to send to the API\n",
        "        model: The model to use (e.g., \"gemini-2.0-flash\", \"gemini-pro\")\n",
        "        temperature: Controls randomness (0=deterministic, 1=creative)\n",
        "        max_retries: Maximum number of retry attempts\n",
        "        retry_delay: Delay between retries in seconds\n",
        "        \n",
        "    Returns:\n",
        "        The completion text\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        print(\"Error: Gemini client is not initialized. Cannot make API call.\")\n",
        "        return \"Error: Gemini client not initialized. Check GEMINI_API_KEY and openai library version.\"\n",
        "            \n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Use the new client.chat.completions.create method\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Error: {e}. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"Failed after {max_retries} attempts: {e}\")\n",
        "                return \"Error: Could not get completion from API\"\n",
        "\n",
        "if client:\n",
        "    print(\"Setup complete! Gemini client is ready.\")\n",
        "else:\n",
        "    print(\"Setup incomplete. Gemini client could not be initialized. Please check API key and library version.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptTemplate:\n",
        "    def __init__(\n",
        "        self, \n",
        "        template: str, \n",
        "        input_variables: List[str] = None,\n",
        "        template_type: str = \"zero-shot\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a prompt template.\n",
        "        \n",
        "        Args:\n",
        "            template: The template string with placeholders for variables\n",
        "            input_variables: List of variable names in the template\n",
        "            template_type: Type of prompt (zero-shot, few-shot, chain-of-thought)\n",
        "        \"\"\"\n",
        "        self.template = template\n",
        "        self.input_variables = input_variables or []\n",
        "        self.template_type = template_type\n",
        "        self.results = []\n",
        "        \n",
        "    def format(self, **kwargs) -> str:\n",
        "        \"\"\"\n",
        "        Format the template with the given variables.\n",
        "        \n",
        "        Args:\n",
        "            **kwargs: The variables to substitute into the template\n",
        "            \n",
        "        Returns:\n",
        "            The formatted prompt\n",
        "        \"\"\"\n",
        "        # Validate that all required variables are provided\n",
        "        for var in self.input_variables:\n",
        "            if var not in kwargs:\n",
        "                raise ValueError(f\"Missing required variable: {var}\")\n",
        "        \n",
        "        # Format the template\n",
        "        return self.template.format(**kwargs)\n",
        "    \n",
        "    def run(\n",
        "        self, \n",
        "        model: str = \"gemini-2.0-flash\", \n",
        "        temperature: float = 0,\n",
        "        **kwargs\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Format the template and get a completion.\n",
        "        \n",
        "        Args:\n",
        "            model: The model to use for completion\n",
        "            temperature: Controls randomness\n",
        "            **kwargs: The variables to substitute into the template\n",
        "            \n",
        "        Returns:\n",
        "            The model's response\n",
        "        \"\"\"\n",
        "        prompt = self.format(**kwargs)\n",
        "        response = get_completion(prompt, model=model, temperature=temperature)\n",
        "        \n",
        "        # Log the result\n",
        "        self.results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response,\n",
        "            \"model\": model,\n",
        "            \"temperature\": temperature,\n",
        "            \"variables\": kwargs\n",
        "        })\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    def get_results_df(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Return the results as a DataFrame.\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame containing all results\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(self.results)\n",
        "\n",
        "# Dictionary to store and compare different prompt templates\n",
        "prompt_templates = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_prompts(\n",
        "    templates: Dict[str, PromptTemplate], \n",
        "    eval_data: List[Dict],\n",
        "    metrics: List[str] = [\"correctness\", \"relevance\", \"clarity\"]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate multiple prompt templates on the same evaluation data.\n",
        "    \n",
        "    Args:\n",
        "        templates: Dictionary of prompt templates to evaluate\n",
        "        eval_data: List of test cases to run\n",
        "        metrics: List of metrics to evaluate\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with evaluation results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for template_name, template in templates.items():\n",
        "        for i, data in enumerate(eval_data):\n",
        "            # Run the template on the test case\n",
        "            response = template.run(**data)\n",
        "            \n",
        "            # Add result entry\n",
        "            result = {\n",
        "                \"template_name\": template_name,\n",
        "                \"template_type\": template.template_type,\n",
        "                \"test_case\": i,\n",
        "                \"response\": response\n",
        "            }\n",
        "            \n",
        "            # Add placeholders for manual metrics\n",
        "            for metric in metrics:\n",
        "                result[f\"{metric}_score\"] = None\n",
        "                \n",
        "            results.append(result)\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def manual_grade(df: pd.DataFrame, metric: str, row_index: int, score: int):\n",
        "    \"\"\"\n",
        "    Manually grade a response.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with evaluation results\n",
        "        metric: The metric to grade (e.g., \"correctness\")\n",
        "        row_index: Index of the row to grade\n",
        "        score: Score to assign (typically 1-5)\n",
        "    \"\"\"\n",
        "    df.at[row_index, f\"{metric}_score\"] = score\n",
        "    return df\n",
        "\n",
        "def visualize_results(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Visualize the evaluation results.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with evaluation results containing scores\n",
        "    \"\"\"\n",
        "    # Group by template type and calculate mean scores\n",
        "    metrics = [col for col in df.columns if col.endswith('_score')]\n",
        "    \n",
        "    if not metrics:\n",
        "        print(\"No metrics found in DataFrame\")\n",
        "        return\n",
        "        \n",
        "    grouped = df.groupby('template_type')[metrics].mean().reset_index()\n",
        "    \n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    x = range(len(grouped['template_type']))\n",
        "    width = 0.8 / len(metrics)\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.bar([pos + i * width for pos in x], grouped[metric], \n",
        "                width=width, label=metric.replace('_score', ''))\n",
        "    \n",
        "    plt.xlabel('Prompt Template Type')\n",
        "    plt.ylabel('Average Score')\n",
        "    plt.title('Prompt Template Performance Comparison')\n",
        "    plt.xticks([pos + 0.4 - width/2 for pos in x], grouped['template_type'])\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Arithmetic Problems\n",
        "\n",
        "#Let's create prompt templates for solving arithmetic problems using different prompting techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot prompting for arithmetic\n",
        "zero_shot_arithmetic = PromptTemplate(\n",
        "    template=\"Solve the following arithmetic problem: {problem}\",\n",
        "    input_variables=[\"problem\"],\n",
        "    template_type=\"zero-shot\"\n",
        ")\n",
        "\n",
        "# Few-shot prompting for arithmetic\n",
        "few_shot_arithmetic = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Solve the following arithmetic problems:\n",
        "\n",
        "Problem: What is 15 + 27?\n",
        "Answer: To solve 15 + 27, I add the numbers: 15 + 27 = 42\n",
        "\n",
        "Problem: What is 8 × 12?\n",
        "Answer: To solve 8 × 12, I multiply the numbers: 8 × 12 = 96\n",
        "\n",
        "Problem: What is 99 - 45?\n",
        "Answer: To solve 99 - 45, I subtract 45 from 99: 99 - 45 = 54\n",
        "\n",
        "Problem: {problem}\n",
        "Answer:\n",
        "\"\"\",\n",
        "    input_variables=[\"problem\"],\n",
        "    template_type=\"few-shot\"\n",
        ")\n",
        "\n",
        "# Chain-of-thought prompting for arithmetic\n",
        "cot_arithmetic = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Solve the following arithmetic problem by breaking it down into steps.\n",
        "\n",
        "Problem: {problem}\n",
        "\n",
        "Let me solve this step-by-step:\n",
        "\"\"\",\n",
        "    input_variables=[\"problem\"],\n",
        "    template_type=\"chain-of-thought\"\n",
        ")\n",
        "\n",
        "# Store the templates\n",
        "prompt_templates[\"zero_shot_arithmetic\"] = zero_shot_arithmetic\n",
        "prompt_templates[\"few_shot_arithmetic\"] = few_shot_arithmetic\n",
        "prompt_templates[\"cot_arithmetic\"] = cot_arithmetic\n",
        "\n",
        "# Test cases for arithmetic\n",
        "arithmetic_test_cases = [\n",
        "    {\"problem\": \"What is 123 + 456?\"},\n",
        "    {\"problem\": \"If 7 people share 35 cookies equally, how many cookies does each person get?\"},\n",
        "    {\"problem\": \"Calculate 15% of 240.\"}\n",
        "]\n",
        "\n",
        "# We'll run the evaluation and show examples of each prompting technique\n",
        "for template_name, template in prompt_templates.items():\n",
        "    if \"arithmetic\" in template_name:\n",
        "        print(f\"Example using {template_name}:\")\n",
        "        response = template.run(**arithmetic_test_cases[0])\n",
        "        print(f\"Problem: {arithmetic_test_cases[0]['problem']}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Text Rephrasing\n",
        "\n",
        "#Creating prompt templates for rephrasing text using different prompting techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot prompting for text rephrasing\n",
        "zero_shot_rephrase = PromptTemplate(\n",
        "    template=\"Rephrase the following text to make it more {style}: {text}\",\n",
        "    input_variables=[\"text\", \"style\"],\n",
        "    template_type=\"zero-shot\"\n",
        ")\n",
        "\n",
        "# Few-shot prompting for text rephrasing\n",
        "few_shot_rephrase = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Here are examples of rephrasing text to make it more {style}:\n",
        "\n",
        "Original: The weather is not good today.\n",
        "{style} version: The meteorological conditions are rather unfavorable at present.\n",
        "\n",
        "Original: I like this book a lot.\n",
        "{style} version: I find this literary work exceedingly appealing.\n",
        "\n",
        "Original: We need to finish this project soon.\n",
        "{style} version: It is imperative that we complete this undertaking in the near future.\n",
        "\n",
        "Now, please rephrase the following text to make it more {style}:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\", \"style\"],\n",
        "    template_type=\"few-shot\"\n",
        ")\n",
        "\n",
        "# Chain-of-thought prompting for text rephrasing\n",
        "cot_rephrase = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "I need to rephrase the following text to make it more {style}.\n",
        "\n",
        "Original text: {text}\n",
        "\n",
        "Let me think about how to rephrase this:\n",
        "1. First, I'll identify the key points and message in the original text.\n",
        "2. Then I'll consider what makes text \"{style}\" - what vocabulary, tone, and structure to use.\n",
        "3. Finally, I'll rewrite the text using those elements while preserving the original meaning.\n",
        "\n",
        "My {style} rephrasing:\n",
        "\"\"\",\n",
        "    input_variables=[\"text\", \"style\"],\n",
        "    template_type=\"chain-of-thought\"\n",
        ")\n",
        "\n",
        "# Store the templates\n",
        "prompt_templates[\"zero_shot_rephrase\"] = zero_shot_rephrase\n",
        "prompt_templates[\"few_shot_rephrase\"] = few_shot_rephrase\n",
        "prompt_templates[\"cot_rephrase\"] = cot_rephrase\n",
        "\n",
        "# Test cases for rephrasing\n",
        "rephrase_test_cases = [\n",
        "    {\n",
        "        \"text\": \"The company has decided to implement a new policy starting next month.\",\n",
        "        \"style\": \"formal\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"This movie wasn't very good and I wouldn't recommend it.\",\n",
        "        \"style\": \"positive\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The research shows that diet affects health outcomes.\",\n",
        "        \"style\": \"academic\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Example of each prompting technique for rephrasing\n",
        "for template_name, template in prompt_templates.items():\n",
        "    if \"rephrase\" in template_name:\n",
        "        print(f\"Example using {template_name}:\")\n",
        "        response = template.run(**rephrase_test_cases[0])\n",
        "        print(f\"Original: {rephrase_test_cases[0]['text']}\")\n",
        "        print(f\"Style: {rephrase_test_cases[0]['style']}\")\n",
        "        print(f\"Rephrased: {response}\")\n",
        "        print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Content Summarization\n",
        "\n",
        "#Creating prompt templates for summarizing content using different prompting techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot prompting for summarization\n",
        "zero_shot_summarize = PromptTemplate(\n",
        "    template=\"Summarize the following text in {word_count} words or less: {text}\",\n",
        "    input_variables=[\"text\", \"word_count\"],\n",
        "    template_type=\"zero-shot\"\n",
        ")\n",
        "\n",
        "# Few-shot prompting for summarization\n",
        "few_shot_summarize = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Here are examples of summarizing text:\n",
        "\n",
        "Original text: The regulations on environmental protection were established to reduce pollution and conserve natural resources. They aim to balance economic growth with sustainable practices. The policy mandates that companies must report their emissions quarterly and invest in cleaner technologies.\n",
        "Summary (25 words): Environmental regulations aim to reduce pollution while balancing economic growth. Companies must report emissions and invest in clean technologies.\n",
        "\n",
        "Original text: The research study examined the effects of different exercise regimens on cardiovascular health in adults aged 40-60. Participants were divided into three groups: high-intensity interval training, moderate continuous exercise, and a control group. After six months, the high-intensity group showed the most significant improvements in heart rate variability and blood pressure.\n",
        "Summary (30 words): A six-month study comparing exercise types found high-intensity interval training produced better cardiovascular improvements than moderate exercise in adults aged 40-60.\n",
        "\n",
        "Now, summarize the following text in {word_count} words or less:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\", \"word_count\"],\n",
        "    template_type=\"few-shot\"\n",
        ")\n",
        "\n",
        "# Chain-of-thought prompting for summarization\n",
        "cot_summarize = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "I need to summarize the following text in {word_count} words or less.\n",
        "\n",
        "Text to summarize: {text}\n",
        "\n",
        "Let me approach this summarization process step by step:\n",
        "1. First, I'll identify the main topic and key points in the text.\n",
        "2. Then, I'll determine which details are essential and which can be omitted.\n",
        "3. Finally, I'll craft a concise summary using clear language while staying under the {word_count} word limit.\n",
        "\n",
        "My summary (within {word_count} words):\n",
        "\"\"\",\n",
        "    input_variables=[\"text\", \"word_count\"],\n",
        "    template_type=\"chain-of-thought\"\n",
        ")\n",
        "\n",
        "# Store the templates\n",
        "prompt_templates[\"zero_shot_summarize\"] = zero_shot_summarize\n",
        "prompt_templates[\"few_shot_summarize\"] = few_shot_summarize\n",
        "prompt_templates[\"cot_summarize\"] = cot_summarize\n",
        "\n",
        "# Test cases for summarization\n",
        "summarize_test_cases = [\n",
        "    {\n",
        "        \"text\": \"\"\"Machine learning is a method of data analysis that automates analytical model building. \n",
        "        It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, \n",
        "        and make decisions with minimal human intervention. The iterative aspect of machine learning is important because \n",
        "        as models are exposed to new data, they are able to independently adapt. They learn from previous computations \n",
        "        to produce reliable, repeatable decisions and results. Machine learning algorithms are often categorized as supervised or unsupervised. \n",
        "        Supervised algorithms require a data scientist or data analyst with machine learning skills to provide both input and desired output, \n",
        "        in addition to furnishing feedback about the accuracy of predictions during algorithm training. Unsupervised algorithms do not need to be \n",
        "        trained with desired outcome data. Instead, they use an iterative approach called deep learning to review data and arrive at conclusions.\"\"\",\n",
        "        \"word_count\": 50\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"\"\"Climate change refers to significant, long-term changes in the global climate. \n",
        "        The global average surface temperature has increased over the last century. This is largely due to human activities, \n",
        "        particularly the burning of fossil fuels, which adds heat-trapping gases to Earth's atmosphere. The effects of human-caused \n",
        "        global warming are apparent and widespread. They include rising sea levels, ecosystem shifts, extreme weather events, \n",
        "        ocean acidification, and potential threats to human health and security. Addressing climate change will require adaptation \n",
        "        to already-occurring impacts and mitigation to reduce future warming. Internationally, the Paris Agreement aims to limit global warming \n",
        "        to well below 2 degrees Celsius above pre-industrial levels.\"\"\",\n",
        "        \"word_count\": 40\n",
        "    }\n",
        "]\n",
        "\n",
        "# Example of each prompting technique for summarization\n",
        "for template_name, template in prompt_templates.items():\n",
        "    if \"summarize\" in template_name:\n",
        "        print(f\"Example using {template_name}:\")\n",
        "        response = template.run(**summarize_test_cases[0])\n",
        "        print(f\"Original length: {len(summarize_test_cases[0]['text'].split())} words\")\n",
        "        print(f\"Target length: {summarize_test_cases[0]['word_count']} words\")\n",
        "        print(f\"Summary: {response}\")\n",
        "        print(f\"Summary length: {len(response.split())} words\")\n",
        "        print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Classification Tasks\n",
        "\n",
        "#Creating prompt templates for classification tasks using different prompting techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot prompting for classification\n",
        "zero_shot_classify = PromptTemplate(\n",
        "    template=\"Classify the following {content_type} into one of these categories: {categories}. Only respond with the category name.\\n\\n{content}\",\n",
        "    input_variables=[\"content\", \"categories\", \"content_type\"],\n",
        "    template_type=\"zero-shot\"\n",
        ")\n",
        "\n",
        "# Few-shot prompting for classification\n",
        "few_shot_classify = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Here are some examples of {content_type} classification into these categories: {categories}\n",
        "\n",
        "Example 1: The customer service was excellent and the staff was very helpful.\n",
        "Category: Positive\n",
        "\n",
        "Example 2: The product arrived damaged and customer service didn't help at all.\n",
        "Category: Negative\n",
        "\n",
        "Example 3: It works as expected, but nothing special about it.\n",
        "Category: Neutral\n",
        "\n",
        "Now, classify the following {content_type} into one of these categories: {categories}. Only respond with the category name.\n",
        "\n",
        "{content}\n",
        "\"\"\",\n",
        "    input_variables=[\"content\", \"categories\", \"content_type\"],\n",
        "    template_type=\"few-shot\"\n",
        ")\n",
        "\n",
        "# Chain-of-thought prompting for classification\n",
        "cot_classify = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "I need to classify the following {content_type} into one of these categories: {categories}.\n",
        "\n",
        "{content_type} to classify: {content}\n",
        "\n",
        "Let me think about this step-by-step:\n",
        "1. First, I'll review what characterizes each category: {categories}.\n",
        "2. Then, I'll analyze the {content_type} to identify key elements that align with a specific category.\n",
        "3. Finally, I'll determine the most appropriate category.\n",
        "\n",
        "Based on my analysis, the category is:\n",
        "\"\"\",\n",
        "    input_variables=[\"content\", \"categories\", \"content_type\"],\n",
        "    template_type=\"chain-of-thought\"\n",
        ")\n",
        "\n",
        "# Store the templates\n",
        "prompt_templates[\"zero_shot_classify\"] = zero_shot_classify\n",
        "prompt_templates[\"few_shot_classify\"] = few_shot_classify\n",
        "prompt_templates[\"cot_classify\"] = cot_classify\n",
        "\n",
        "# Test cases for classification\n",
        "classify_test_cases = [\n",
        "    {\n",
        "        \"content\": \"I'm not sure if I like this product. It has some good features but also some issues.\",\n",
        "        \"categories\": \"Positive, Negative, Neutral\",\n",
        "        \"content_type\": \"product review\"\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Scientists discover new species of deep-sea fish that can survive extreme pressure.\",\n",
        "        \"categories\": \"Science, Politics, Entertainment, Sports, Technology\",\n",
        "        \"content_type\": \"news headline\"\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"The company's revenue increased by 15% in the last quarter due to new product launches.\",\n",
        "        \"categories\": \"Financial Report, Marketing, Product Development, Human Resources\",\n",
        "        \"content_type\": \"business statement\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Example of each prompting technique for classification\n",
        "for template_name, template in prompt_templates.items():\n",
        "    if \"classify\" in template_name:\n",
        "        print(f\"Example using {template_name}:\")\n",
        "        response = template.run(**classify_test_cases[0])\n",
        "        print(f\"Content: {classify_test_cases[0]['content']}\")\n",
        "        print(f\"Categories: {classify_test_cases[0]['categories']}\")\n",
        "        print(f\"Classification: {response}\")\n",
        "        print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Prompt Performance Evaluation\n",
        "\n",
        "Now let's run a comprehensive evaluation of our prompt templates and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up dictionaries for each task type\n",
        "arithmetic_templates = {\n",
        "    name: template for name, template in prompt_templates.items() \n",
        "    if \"arithmetic\" in name\n",
        "}\n",
        "\n",
        "rephrase_templates = {\n",
        "    name: template for name, template in prompt_templates.items() \n",
        "    if \"rephrase\" in name\n",
        "}\n",
        "\n",
        "summarize_templates = {\n",
        "    name: template for name, template in prompt_templates.items() \n",
        "    if \"summarize\" in name\n",
        "}\n",
        "\n",
        "classify_templates = {\n",
        "    name: template for name, template in prompt_templates.items() \n",
        "    if \"classify\" in name\n",
        "}\n",
        "\n",
        "# Run evaluations for each task type\n",
        "print(\"Running evaluations for all tasks...\")\n",
        "\n",
        "# Arithmetic evaluation\n",
        "arithmetic_results = evaluate_prompts(\n",
        "    arithmetic_templates, \n",
        "    arithmetic_test_cases,\n",
        "    metrics=[\"correctness\", \"clarity\", \"efficiency\"]\n",
        ")\n",
        "\n",
        "# Rephrasing evaluation\n",
        "rephrase_results = evaluate_prompts(\n",
        "    rephrase_templates, \n",
        "    rephrase_test_cases,\n",
        "    metrics=[\"style_adherence\", \"clarity\", \"creativity\"]\n",
        ")\n",
        "\n",
        "# Summarization evaluation\n",
        "summarize_results = evaluate_prompts(\n",
        "    summarize_templates, \n",
        "    summarize_test_cases,\n",
        "    metrics=[\"conciseness\", \"completeness\", \"accuracy\"]\n",
        ")\n",
        "\n",
        "# Classification evaluation\n",
        "classify_results = evaluate_prompts(\n",
        "    classify_templates, \n",
        "    classify_test_cases,\n",
        "    metrics=[\"accuracy\", \"confidence\", \"reasoning\"]\n",
        ")\n",
        "\n",
        "print(\"Evaluations complete! You can now manually grade the results using the manual_grade function.\")\n",
        "print(\"Example: manual_grade(arithmetic_results, 'correctness', 0, 5)\")\n",
        "\n",
        "# Combine all results\n",
        "all_results = pd.concat([\n",
        "    arithmetic_results, \n",
        "    rephrase_results, \n",
        "    summarize_results, \n",
        "    classify_results\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of manual grading (you would grade each response in practice)\n",
        "# For demonstration purposes, we'll grade a few examples\n",
        "\n",
        "# Grade arithmetic results (simulating manual grading)\n",
        "for i in range(min(3, len(arithmetic_results))):\n",
        "    # Simulating scores based on template type - in real use, you would review responses and grade them\n",
        "    if \"zero_shot\" in arithmetic_results.iloc[i]['template_name']:\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'correctness', i, 3)\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'clarity', i, 2)\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'efficiency', i, 2)\n",
        "    elif \"few_shot\" in arithmetic_results.iloc[i]['template_name']:\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'correctness', i, 4)\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'clarity', i, 4)\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'efficiency', i, 3)\n",
        "    else:  # chain-of-thought\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'correctness', i, 5)\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'clarity', i, 5)\n",
        "        arithmetic_results = manual_grade(arithmetic_results, 'efficiency', i, 4)\n",
        "\n",
        "# Visualize the arithmetic results\n",
        "arithmetic_plot = visualize_results(arithmetic_results)\n",
        "arithmetic_plot.title(\"Arithmetic Prompt Performance\")\n",
        "display(arithmetic_plot.figure)\n",
        "\n",
        "# In a real evaluation, you would continue by grading all other task types:\n",
        "# - manual_grade(rephrase_results, ...)\n",
        "# - manual_grade(summarize_results, ...)\n",
        "# - manual_grade(classify_results, ...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Prompt Logs and Version Control\n",
        "\n",
        "#This section demonstrates how to log prompt templates and results for version control.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_prompt_logs(templates, results, version=\"v1\", path=\"prompt_logs\"):\n",
        "    \"\"\"\n",
        "    Save prompt templates and results to files for version control.\n",
        "    \n",
        "    Args:\n",
        "        templates: Dictionary of prompt templates\n",
        "        results: DataFrame with evaluation results\n",
        "        version: Version string\n",
        "        path: Directory to save logs\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "    # Save templates\n",
        "    template_logs = {}\n",
        "    for name, template in templates.items():\n",
        "        template_logs[name] = {\n",
        "            \"template\": template.template,\n",
        "            \"input_variables\": template.input_variables,\n",
        "            \"template_type\": template.template_type\n",
        "        }\n",
        "    \n",
        "    with open(f\"{path}/templates_{version}.json\", \"w\") as f:\n",
        "        json.dump(template_logs, f, indent=2)\n",
        "    \n",
        "    # Save results\n",
        "    results_file = f\"{path}/results_{version}.csv\"\n",
        "    results.to_csv(results_file, index=False)\n",
        "    \n",
        "    print(f\"Saved templates to {path}/templates_{version}.json\")\n",
        "    print(f\"Saved results to {path}/results_{version}.csv\")\n",
        "    \n",
        "    # Generate report\n",
        "    report = f\"\"\"# Prompt Engineering Report {version}\n",
        "\n",
        "## Summary\n",
        "- Total templates: {len(templates)}\n",
        "- Total evaluations: {len(results)}\n",
        "- Template types: {', '.join(results['template_type'].unique())}\n",
        "\n",
        "## Tasks Evaluated\n",
        "- {', '.join(set([name.split('_')[-1] for name in templates.keys()]))}\n",
        "\"\"\"\n",
        "    \n",
        "    with open(f\"{path}/report_{version}.md\", \"w\") as f:\n",
        "        f.write(report)\n",
        "    \n",
        "    print(f\"Generated report at {path}/report_{version}.md\")\n",
        "\n",
        "# Create logs directory\n",
        "os.makedirs(\"week_two/prompt_logs\", exist_ok=True)\n",
        "\n",
        "# Save the prompt templates and evaluation results\n",
        "save_prompt_logs(\n",
        "    prompt_templates, \n",
        "    all_results, \n",
        "    version=\"v1\", \n",
        "    path=\"week_two/prompt_logs\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## 8. Conclusion\n",
        "\n",
        "# This notebook has demonstrated:\n",
        "\n",
        "# 1. **Different prompting techniques**:\n",
        "#    - Zero-shot prompting: Direct instructions with no examples\n",
        "#    - Few-shot prompting: Including examples to guide the model\n",
        "#    - Chain-of-thought prompting: Encouraging step-by-step reasoning\n",
        "\n",
        "# 2. **Application across diverse tasks**:\n",
        "#    - Arithmetic problems\n",
        "#    - Text rephrasing\n",
        "#    - Content summarization\n",
        "#    - Classification\n",
        "\n",
        "# 3. **Structured approach to prompt engineering**:\n",
        "#    - Created a reusable `PromptTemplate` class\n",
        "#    - Implemented evaluation framework\n",
        "#    - Set up version control for prompt experiments\n",
        "\n",
        "# 4. **Key findings**:\n",
        "#    - Chain-of-thought prompting generally produces more detailed and accurate responses\n",
        "#    - Few-shot prompting is valuable for setting format expectations\n",
        "#    - Zero-shot works well for simple tasks but may lack consistency\n",
        "\n",
        "# For real-world applications, continue experimenting with different prompting strategies and adapt them to your specific use cases. Keep in mind that the effectiveness of each technique may vary depending on the model being used and the complexity of the task.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
