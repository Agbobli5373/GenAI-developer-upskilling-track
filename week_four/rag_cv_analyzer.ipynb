{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681f7bef",
   "metadata": {},
   "source": [
    "# RAG System for CV Analysis\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using LangChain and HuggingFace to analyze CV/Resume documents. The system allows you to ask natural language questions about a CV and get relevant answers with source citations.\n",
    "\n",
    "## Features:\n",
    "- PDF document loading and processing\n",
    "- Text chunking and embedding generation\n",
    "- Vector similarity search with FAISS\n",
    "- Question-answering with source attribution\n",
    "- Interactive Q&A session\n",
    "\n",
    "## Requirements:\n",
    "- LangChain community packages\n",
    "- HuggingFace Transformers\n",
    "- FAISS vector store\n",
    "- PyMuPDF for PDF processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511881c7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our RAG system. We'll also install python-dotenv for secure environment variable management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community transformers faiss-cpu pymupdf sentence-transformers python-dotenv\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Optional: Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # This loads variables from .env file if it exists\n",
    "    print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  python-dotenv not available. Using system environment variables only.\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4945",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Define the configuration parameters for our RAG system:\n",
    "\n",
    "**üîí Security Best Practices:**\n",
    "- **Never hardcode API tokens** in notebooks or code\n",
    "- Use environment variables, config files, or secure vaults\n",
    "- Add sensitive files to `.gitignore`\n",
    "- Use different methods based on your environment\n",
    "\n",
    "**Choose one of the following methods:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Choose ONE of the following methods based on your environment:\n",
    "\n",
    "# METHOD 1: Environment Variables (Recommended for production)\n",
    "# Set these in your system environment or .env file\n",
    "PDF_PATH = os.getenv('PDF_PATH', './sample_cv.pdf')  # Default fallback\n",
    "HF_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# METHOD 2: Load from .env file (Recommended for development)\n",
    "# Uncomment the following if you're using python-dotenv:\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# HF_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# METHOD 3: Load from config file (Alternative approach)\n",
    "# config_file = 'config.json'  # Create this file separately\n",
    "# if os.path.exists(config_file):\n",
    "#     import json\n",
    "#     with open(config_file, 'r') as f:\n",
    "#         config = json.load(f)\n",
    "#     HF_API_TOKEN = config.get('huggingface_token')\n",
    "#     PDF_PATH = config.get('pdf_path', './sample_cv.pdf')\n",
    "\n",
    "# METHOD 4: Interactive input (For testing/demo purposes)\n",
    "# HF_API_TOKEN = input(\"Enter your HuggingFace API token: \")\n",
    "# PDF_PATH = input(\"Enter path to your CV PDF: \")\n",
    "\n",
    "# Set the API token in environment\n",
    "if HF_API_TOKEN:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_API_TOKEN\n",
    "    print(\"‚úÖ HuggingFace API token loaded successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No HuggingFace API token found. Please set HUGGINGFACEHUB_API_TOKEN environment variable.\")\n",
    "    print(\"   You can also create a .env file or use one of the other methods above.\")\n",
    "\n",
    "# Model configurations (these can be public)\n",
    "HF_LLM_MODEL = \"google/flan-t5-small\"  # Language model for Q&A\n",
    "HF_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Embedding model\n",
    "\n",
    "print(f\"\\nüìÅ Configuration Summary:\")\n",
    "print(f\"PDF Path: {PDF_PATH}\")\n",
    "print(f\"LLM Model: {HF_LLM_MODEL}\")\n",
    "print(f\"Embedding Model: {HF_EMBEDDING_MODEL}\")\n",
    "print(f\"API Token: {'‚úÖ Set' if HF_API_TOKEN else '‚ùå Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170ac39",
   "metadata": {},
   "source": [
    "### üîß Setup Instructions for Each Method\n",
    "\n",
    "#### Method 1: Environment Variables (Recommended)\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "# Set temporarily (current session only)\n",
    "set HUGGINGFACEHUB_API_TOKEN=your_token_here\n",
    "set PDF_PATH=C:/path/to/your/cv.pdf\n",
    "\n",
    "# Set permanently\n",
    "setx HUGGINGFACEHUB_API_TOKEN \"your_token_here\"\n",
    "setx PDF_PATH \"C:/path/to/your/cv.pdf\"\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "# Add to ~/.bashrc or ~/.zshrc\n",
    "export HUGGINGFACEHUB_API_TOKEN=\"your_token_here\"\n",
    "export PDF_PATH=\"/path/to/your/cv.pdf\"\n",
    "\n",
    "# Then reload: source ~/.bashrc\n",
    "```\n",
    "\n",
    "#### Method 2: .env File (Development)\n",
    "\n",
    "1. Install python-dotenv: `pip install python-dotenv`\n",
    "2. Create `.env` file in your project root:\n",
    "```\n",
    "HUGGINGFACEHUB_API_TOKEN=your_token_here\n",
    "PDF_PATH=./your_cv.pdf\n",
    "```\n",
    "3. Add `.env` to your `.gitignore` file\n",
    "4. Uncomment the dotenv lines in the code above\n",
    "\n",
    "#### Method 3: Config File\n",
    "\n",
    "1. Create `config.json` (add to `.gitignore`):\n",
    "```json\n",
    "{\n",
    "    \"huggingface_token\": \"your_token_here\",\n",
    "    \"pdf_path\": \"./your_cv.pdf\"\n",
    "}\n",
    "```\n",
    "2. Uncomment the config file lines in the code above\n",
    "\n",
    "#### Method 4: Interactive Input\n",
    "- Good for demos or one-time use\n",
    "- Token won't be saved anywhere\n",
    "- Uncomment the input lines in the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP HELPER: Create .env file template ===\n",
    "# Run this cell to create a template .env file\n",
    "\n",
    "env_template = \"\"\"# HuggingFace API Token - Get from https://huggingface.co/settings/tokens\n",
    "HUGGINGFACEHUB_API_TOKEN=your_token_here\n",
    "\n",
    "# Path to your CV PDF file\n",
    "PDF_PATH=./sample_cv.pdf\n",
    "\n",
    "# Optional: Other configurations\n",
    "# HF_LLM_MODEL=google/flan-t5-small\n",
    "# HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\"\"\"\n",
    "\n",
    "# Create .env template file\n",
    "with open('.env.template', 'w') as f:\n",
    "    f.write(env_template)\n",
    "\n",
    "print(\"‚úÖ Created .env.template file\")\n",
    "print(\"üìù Steps to use:\")\n",
    "print(\"   1. Copy .env.template to .env\")\n",
    "print(\"   2. Edit .env and add your actual token\")\n",
    "print(\"   3. Install python-dotenv: pip install python-dotenv\")\n",
    "print(\"   4. Uncomment the dotenv lines in the configuration cell above\")\n",
    "\n",
    "# Create .gitignore if it doesn't exist\n",
    "gitignore_content = \"\"\"# Environment variables\n",
    ".env\n",
    "config.json\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists('.gitignore'):\n",
    "    with open('.gitignore', 'w') as f:\n",
    "        f.write(gitignore_content)\n",
    "    print(\"‚úÖ Created .gitignore file\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  .gitignore already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c74209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECURITY VALIDATION ===\n",
    "# This cell checks if you're following security best practices\n",
    "\n",
    "def validate_security():\n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check if token is hardcoded in this cell\n",
    "    if 'hf_' in str(locals()) or 'hf_' in str(globals()):\n",
    "        issues.append(\"üö® Potential hardcoded token detected in variables\")\n",
    "    \n",
    "    # Check if .env exists and is in gitignore\n",
    "    if os.path.exists('.env'):\n",
    "        if os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'r') as f:\n",
    "                gitignore_content = f.read()\n",
    "            if '.env' not in gitignore_content:\n",
    "                issues.append(\"üö® .env file exists but not in .gitignore\")\n",
    "        else:\n",
    "            issues.append(\"üö® .env file exists but no .gitignore found\")\n",
    "    \n",
    "    # Check environment variable\n",
    "    if os.getenv('HUGGINGFACEHUB_API_TOKEN'):\n",
    "        if os.getenv('HUGGINGFACEHUB_API_TOKEN').startswith('hf_'):\n",
    "            print(\"‚úÖ Valid HuggingFace token format detected\")\n",
    "        else:\n",
    "            warnings.append(\"‚ö†Ô∏è  Token format doesn't match expected HuggingFace format\")\n",
    "    else:\n",
    "        warnings.append(\"‚ö†Ô∏è  No HUGGINGFACEHUB_API_TOKEN environment variable found\")\n",
    "    \n",
    "    # Print results\n",
    "    if not issues and not warnings:\n",
    "        print(\"üéâ All security checks passed!\")\n",
    "    else:\n",
    "        if issues:\n",
    "            print(\"üö® SECURITY ISSUES FOUND:\")\n",
    "            for issue in issues:\n",
    "                print(f\"   {issue}\")\n",
    "        if warnings:\n",
    "            print(\"‚ö†Ô∏è  WARNINGS:\")\n",
    "            for warning in warnings:\n",
    "                print(f\"   {warning}\")\n",
    "    \n",
    "    return len(issues) == 0\n",
    "\n",
    "validate_security()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d10d53",
   "metadata": {},
   "source": [
    "## 3. Define RAG System Builder Function\n",
    "\n",
    "This function builds the complete RAG system pipeline:\n",
    "1. **Document Loading**: Load PDF using PyMuPDFLoader\n",
    "2. **Text Splitting**: Break document into manageable chunks\n",
    "3. **Embeddings**: Generate vector embeddings for semantic search\n",
    "4. **Vector Store**: Create FAISS index for similarity search\n",
    "5. **LLM Setup**: Initialize the language model pipeline\n",
    "6. **QA Chain**: Configure the retrieval-based question answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca02e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_system(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Builds an improved RAG system with better token management and prompting.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting RAG System Build for: {pdf_path} ---\")\n",
    "\n",
    "    # 1. Document Loader\n",
    "    try:\n",
    "        print(\"Loading document with PyMuPDFLoader...\")\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} pages from the PDF.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Text Splitting with smaller chunks for better token management\n",
    "    print(\"Splitting documents into smaller chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # Reduced chunk size\n",
    "        chunk_overlap=100,  # Reduced overlap\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(chunks)} text chunks.\")\n",
    "\n",
    "    # 3. Embedding\n",
    "    print(f\"Generating embeddings using: {HF_EMBEDDING_MODEL}...\")\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=HF_EMBEDDING_MODEL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 4. Vector Store\n",
    "    print(\"Creating FAISS vector store...\")\n",
    "    try:\n",
    "        vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "        print(\"FAISS vector store created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 5. LLM Setup with better configuration\n",
    "    print(f\"Initializing LLM with model: {HF_LLM_MODEL}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(HF_LLM_MODEL)\n",
    "        \n",
    "        # Better pipeline configuration\n",
    "        pipe = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=150,  # Limit new tokens generated\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        print(\"LLM initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 6. Custom prompt template for better responses\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, \n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # 7. Create RetrievalQA chain with custom prompt\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}  # Limit to 2 most relevant chunks\n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    print(\"RetrievalQA chain configured with custom prompt.\")\n",
    "    print(\"--- RAG System Build Complete ---\")\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de96c29",
   "metadata": {},
   "source": [
    "## 4. Define Answer Formatting Function\n",
    "\n",
    "This function formats the RAG system's responses in a user-friendly way, showing both the answer and the source documents used to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_answer(result):\n",
    "    \"\"\"\n",
    "    Format the answer in a more readable way.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Answer ---\")\n",
    "    answer = result[\"result\"].strip()\n",
    "    if answer:\n",
    "        print(answer)\n",
    "    else:\n",
    "        print(\"I couldn't find a specific answer to your question.\")\n",
    "    \n",
    "    print(\"\\n--- Source Information ---\")\n",
    "    if \"source_documents\" in result:\n",
    "        for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "            page_num = doc.metadata.get('page', 'Unknown')\n",
    "            print(f\"Source {i} (Page {page_num}):\")\n",
    "            # Show more content for context\n",
    "            content = doc.page_content.strip()\n",
    "            print(f\"  {content[:300]}...\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No source documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d4765",
   "metadata": {},
   "source": [
    "## 5. Main Execution Logic\n",
    "\n",
    "Now let's build the RAG system and start the interactive Q&A session. \n",
    "\n",
    "**Before running this cell:**\n",
    "1. Make sure the PDF path in the configuration is correct\n",
    "2. Ensure you have a valid HuggingFace API token\n",
    "3. The PDF file should be accessible from the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PDF file exists\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    print(f\"Error: PDF file not found at '{PDF_PATH}'.\")\n",
    "    print(\"Please update the PDF_PATH in the configuration section.\")\n",
    "else:\n",
    "    print(\"PDF file found. Building RAG system...\")\n",
    "    qa_system = build_rag_system(PDF_PATH)\n",
    "    \n",
    "    if qa_system:\n",
    "        print(\"\\n--- RAG System Ready ---\")\n",
    "        print(\"You can now ask questions about the CV document.\")\n",
    "        print(\"\\nSample questions you can ask:\")\n",
    "        print(\"- Who is the candidate?\")\n",
    "        print(\"- What are the candidate's skills?\")\n",
    "        print(\"- What is the candidate's education background?\")\n",
    "        print(\"- What projects has the candidate worked on?\")\n",
    "        print(\"- What programming languages does the candidate know?\")\n",
    "        print(\"- What is the candidate's work experience?\")\n",
    "    else:\n",
    "        print(\"\\nFailed to initialize RAG system.\")\n",
    "        print(\"Please check the error messages above and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af4342",
   "metadata": {},
   "source": [
    "## 6. Interactive Q&A Session\n",
    "\n",
    "Run the following cells to ask questions about the CV. Each cell demonstrates a different type of query you can make to the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db647065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 1: Ask about the candidate's identity\n",
    "if 'qa_system' in locals() and qa_system:\n",
    "    question = \"Who is the candidate? What is their name and professional title?\"\n",
    "    print(f\"Question: {question}\")\n",
    "    try:\n",
    "        result = qa_system.invoke({\"query\": question})\n",
    "        format_answer(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "else:\n",
    "    print(\"RAG system not initialized. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 2: Ask about technical skills\n",
    "if 'qa_system' in locals() and qa_system:\n",
    "    question = \"What are the candidate's technical skills and programming languages?\"\n",
    "    print(f\"Question: {question}\")\n",
    "    try:\n",
    "        result = qa_system.invoke({\"query\": question})\n",
    "        format_answer(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "else:\n",
    "    print(\"RAG system not initialized. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Query: Ask your own question\n",
    "if 'qa_system' in locals() and qa_system:\n",
    "    # You can modify this question to ask anything about the CV\n",
    "    custom_question = \"What is the candidate's education background?\"\n",
    "    \n",
    "    print(f\"Custom Question: {custom_question}\")\n",
    "    try:\n",
    "        result = qa_system.invoke({\"query\": custom_question})\n",
    "        format_answer(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"To ask a different question, modify the 'custom_question' variable above and run this cell again.\")\n",
    "else:\n",
    "    print(\"RAG system not initialized. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2accb",
   "metadata": {},
   "source": [
    "## 7. Continuous Interactive Session\n",
    "\n",
    "For a more interactive experience, run the following cell to start a continuous Q&A session where you can ask multiple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee243ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Interactive Session\n",
    "if 'qa_system' in locals() and qa_system:\n",
    "    print(\"Starting interactive Q&A session...\")\n",
    "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
    "    print(\"Type 'help' to see sample questions.\")\n",
    "    \n",
    "    sample_questions = [\n",
    "        \"Who is the candidate?\",\n",
    "        \"What are the candidate's skills?\",\n",
    "        \"What is the candidate's education background?\",\n",
    "        \"What projects has the candidate worked on?\",\n",
    "        \"What programming languages does the candidate know?\",\n",
    "        \"What is the candidate's work experience?\",\n",
    "        \"What certifications does the candidate have?\",\n",
    "        \"What are the candidate's achievements?\"\n",
    "    ]\n",
    "    \n",
    "    session_active = True\n",
    "    question_count = 0\n",
    "    \n",
    "    while session_active and question_count < 10:  # Limit to 10 questions for notebook\n",
    "        user_query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
    "        \n",
    "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Exiting RAG Q&A session. Goodbye!\")\n",
    "            session_active = False\n",
    "            break\n",
    "            \n",
    "        if user_query.lower() == \"help\":\n",
    "            print(\"\\nSample questions you can ask:\")\n",
    "            for i, q in enumerate(sample_questions, 1):\n",
    "                print(f\"{i}. {q}\")\n",
    "            continue\n",
    "            \n",
    "        if user_query.strip():\n",
    "            try:\n",
    "                print(\"Querying RAG system...\")\n",
    "                result = qa_system.invoke({\"query\": user_query})\n",
    "                format_answer(result)\n",
    "                question_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during query: {e}\")\n",
    "                print(\"Please try rephrasing your question.\")\n",
    "        else:\n",
    "            print(\"Please enter a non-empty question.\")\n",
    "    \n",
    "    if question_count >= 10:\n",
    "        print(\"\\nReached maximum number of questions for this session.\")\n",
    "        print(\"To continue, restart this cell.\")\n",
    "else:\n",
    "    print(\"RAG system not initialized. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019b185",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully built and tested a RAG system for CV analysis. Here are some ways to improve and extend this system:\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Better Models**: Try larger language models like `google/flan-t5-base` or `google/flan-t5-large`\n",
    "2. **Enhanced Chunking**: Experiment with different chunk sizes and overlap values\n",
    "3. **Multiple Documents**: Extend to handle multiple CVs for comparison\n",
    "4. **Structured Output**: Parse CV sections (education, experience, skills) separately\n",
    "5. **Semantic Search**: Add filters for specific CV sections\n",
    "6. **Performance Optimization**: Implement caching for frequently asked questions\n",
    "\n",
    "### Usage Tips:\n",
    "- For better results, ask specific questions rather than general ones\n",
    "- The system works best with well-structured PDF documents\n",
    "- Try different phrasings if you don't get the expected answer\n",
    "- Check the source documents to understand how the answer was derived\n",
    "\n",
    "### Common Issues:\n",
    "- **PDF Loading Errors**: Ensure the PDF path is correct and accessible\n",
    "- **Model Loading Issues**: Check your internet connection and HuggingFace token\n",
    "- **Memory Issues**: For large documents, consider reducing chunk size or using a smaller embedding model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
